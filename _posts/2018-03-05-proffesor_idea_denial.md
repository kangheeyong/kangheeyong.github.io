---
title: 두개의 아이디어를 논리적으로 부정 시도(만족 못 함...)
description:
categories:
 - study
tags:
---

### deep learning 학습속도 향상에 대한 아이디어

이 아이디어는 다음과 같다.

    deep learning은 많은 데이터를 사용하기 때문에 학습하는데 속도가 오래 걸린다. 만약 k-maens와 같은 알고리즘으로 전체 데이터 중 일부분을 추출한 데이터로 학습해서 네트워크의 파라메터를 어느정도 학습 한 후에 전체 데이터로 학습한 것이 처음부터 전체 데이터로 학습한 것 보다 더 빠를 것이다.

먼저 일부 데이터를 추출해서 학습해서 네트워크의 파라메터를 어느정도 학습한다는 의미부터 해석해봐야 할 것 같다. 보통 deep learning은 확률 모델 기반으로 네트워크를 해석한다. 따라서 확률 기반으로 해석한다면,

1. 적은 데이터는 전체 데이터와 비슷한 확률 분포를 따를 것이다.
2. 비슷한 확률 분포를 따른다는 것은 적은 데이터가 전체 데이터의 성향?을 어느정도 대표?할 수 있는 값이라고 생각 할 수 있다.
3. deep learning은 확률 분포를 학습한다.
4. 데이터가 적으면 deep learning은 좀 더 빨리 수렴한다.
5. 하지만 비슷한 데이터기 때문에 비슷하게만 학습한다.(pre-training 효과)
6. 비슷한 확률 분포로 이미 학습했기 때문에 전체 데이터로 다시 학습한다면 조금만 학습해도 금방 수렴할 것이다.

위의 아이디어를 낸 사람은 아마도 위와 같은 논리 때문에 학습 속도가 향상된다고 했다. 이제 나의 부족한 논리력으로... 차근차근 무너뜨려 보겠다...

#### 적은 데이터는 전체 데이터와 비슷한 확률 분포를 따를 것이다.

일단 이 문제에서는 어떤 종류의 데이터라는게 명시되지 않는다. 하지만 보통 deep learning에서 주로 연구되는 분야는 이미지 또는 음성 신호이다. 일단 k-means로 추출한 데이터가 전체 데이터의 확률 분포를 따른다는 논리적 근거를 찾지 못했다. 따라서 반례를 들어 위의 문장을 부정 하겠다.

   ![](/assets/2018-03-01/1.png)

[k-means 위키피디아 한계점(위의 그림)](https://ko.wikipedia.org/wiki/K-평균_알고리즘)을 보면 k의 수가 많을 경우(위의 문제의 경우 이상적인 클러수터의 수보다 k를 많게 설정해서 알고리즘을 실행시킨 결과를 의미한다.) 데이터 포인터가 몰려 있는 곳이 아닌 sparse한 부분(우리가 원하는 대표값이 아니다.)에 클러스터 포인터가 존재하는 것을 확인 할 수 있다. 따라서 k-means 알고리즘에서 나올 결과값이 전체 확률 분포를 비슷하게 유지하면서 추출한다는 것은 참이 아니다.

    k-means 알고리즘으로 추출한 데이터가 원래 분포를 따르지 않는 반례를 보여 항상 참이 아니라는 것을 보였다. 물론 비슷하게 따른다는 말이 모호하긴 하다.

무작위 추출을 생각해보면 어떨까? k-means 알고리즘으로 추출한 결과와 다르게 무작위 추출의 경우 [표본평균의 분포](https://ko.wikipedia.org/wiki/표본평균의_분포)를 따른다는 이론이 존재한다. 기본적으로 우리는 어떤 데이터의 확률 분포를 쉽게 구할 수 없다. 하지만 무작위 추출의 경우 데이터의 종류와 상관 없이 어느정도 근사화된 데이터로 추출 된다는 나름데로 이론적 근거가 있다.


**결론 : k-means로 추출한 데이터가 전체 데이터의 확률 분포를 따른다고 가정하는 것은 무리수인 것 같다.**



    약 반년 전에 이 아이디어에 대한 실험을 해보았다. k-means 알고리즘으로 추출한 데이터와 무작위 추출로 추출한 데이터로 deep learning 알고리즘으로 학습해 보았으나 랜덤 추출이 더 높은 정확도의 결과를 얻을 수 있었다.(pre-training 으로서의 결과이다.) 그 때 당시에는 왜 이런지 이유를 몰랐지만 현제 내린 결론은 k-means로 추출한 데이터의 확률 분포가 원래와 다르게 분포하는게 아닐까 생각한다.


#### 하지만 비슷한 데이터기 때문에 비슷하게만 학습한다.(pre-training 효과)

k-means가 원래 확률 분포를 항상 따르지 않을 경우가 있다는 것을 알았다. 하지만 어떤 조건이 충족되어 비슷하게 확률 분포를 따를 때가 있지 않을까 생각한다. 그렇다면 무작위 추출과 같은 효과라고 생각한다. 비록 오차의 범위가 무작위 추출이 큰지 낮은지 알 수 없지만, 이론적으로 확실한 무작위 추출을 가지고 다음 과정을 설명하겠다.

설명하기 앞서 추출한 데이터가 원래 분포와 완전히 같은 경우와 다른 경우를 나누어서 생각해 보겠다.

   ![](/assets/2018-03-01/2.png)

전체 데이터 집합을 대문자 $$S$$로 표기하고 각각의 추출한 경우의 집합을 소문자 $$s$$로 표기한다.

1. 상황 : 추출한 데이터가 완벽하게 원래 분포와 같은 경우(비슷하다면서 원래 분포와 거의 같은 경우)

학습 데이터가 들어가는 순서를 수식으로 나타낸다면


$$(s_1 \times n) + (\sum S) $$

앞의 괄호는 무작위 추출 후 학습에 사용한 데이터이고 뒤의 괄호는 전체 데이터 학습에 사용한 데이터이다. 여기서 위의 상황을 분석해보면 전체 데이터 $$S$$의 분포와 무작위 추출의 분포 $$s_1$$의 분포는 같다. 따라서 $$s_1$$분포를 $$n$$번 사용 했으면 전체 데이터 $$S$$를 한 번 사용한 것과 같은 효과이다. 따라서 위의 식은

$$(S) + (\sum S) = \sum S $$

로 바뀌게 되고, 이 경우는 그냥 전체 데이터를 사용해서 학습하는 것과 같은 결과를 얻는다고 예측할 수 있다.

**결과 분석 : 추출한 데이터가 원래 분포와 가까울 수록 그냥 전체를 가지고 학습하는 것과 차이가 없다는 것을 추측할 수 있다.**


2. 상황 : 추출한 데이터가 원래 분포와 다른 경우(비슷하다지만 원래 분포와 다른 경우)

$$(s_1 \times n) + (\sum S) $$

$$(s_1 \times n)$$부분으로 학습한 결과는 원래 분포와 다른 분포로 학습할 것이다. 보통 deep learning의 파라메터 초기값은 랜덤으로 한다. 원래와 다른 분포로 학습된 파라메터는 어떻게 보면 랜텀으로 초기화 한 것과 같다고 생각할 수 있다.(수많은 경우 중 하나라고 생각한다면?) 그렇다면 추출한 데이터로 학습을 해서 파라메터를 어느정도 학습한다는 것은 파라메터를 랜덤으로 초기화 한 번 하는 것과 같은 결과를 얻는다고 생각할 수 있다.  

**결과 분석 : 추출한 데이터와 원래 분포와 다면 다를수록 랜덤 초기화 효과와  거의 같아지는 효과가 된다고 추축할 수 있다.**

#### 최종 결론(내가 생각한 가장 그럴싸한 결론)

가장 이상적인 경우는 추출한 데이터가 원래 분포와 같은 경우일 것이다. 하지만 이 경우 전체 데이터로 그냥 학습하는 것과 같다는 결론이 나온다. 만약 그저 비슷한 경우 원래 분포와 거의 같을 수도 있지만 다를 수도 있다. 이 경우 그저 랜덤으로 초기화 한 파라메터와 별 차이가 없다고 추측한다. 우리가 원하는 결과는 항상 성능이 좋아야 하지만 여기서 얻어낸 결론은 기존 방법과 차이가 없다는 결론(상황 1)과 아무것도 추측 할 수 없는 결론(상황 2)을 얻었다.

만약 해야할 실험이 여러게라면 위의 아이디어는 우선순위를 최하로 하는 것이 가장 합리적인 선택이 아닐까 생각한다.

    사실 실험을 해보았다... 그런데 잘 생각해보면 k-means로 추출하던 랜덤 추출을 하던 코드를 추가 해야하고 별도로 이어서 학습하는 코드도 추가해야 한다. 이러한 별로의 연산들이 포함되기 때문에 당연히 위의 아이디어 방법은 기본의 방식보다 더 느렸다...

#### 고찰

처음 구상은 아래와 같았다.

   ![](/assets/2018-03-01/3.png)

그리고 글로 정리하면서 여러번 글을 썼다 지웠다를 반복했다. 이미지로 구성했을 때는 뭔가 다 맞는 생각인 것 처럼 보였으나 글로 쓰면서 내용을 전개하니 논리적으로 헛점이 많았다. 그래서 위와 같이 부실한 분석이 나왔다. 그리고 사실 가장 이상적인 것은 위의 아이디어를 듣고 분석한 뒤 실험을 하고 결과를 확인하는 것이 가장 이상적인 실험이 아닐까 생각하지만 현실은 실험 결과보고 뭔가 그 이유를 억지로 끼워 맞춘 느낌이 든다...   

### haze removal(안개 제거) for deep learning 에 대한 아이디어

이 아이디어는 다음과 같다.

    안개 제거 문제를 deep learning에 적용하기 위해서는 안개가 제거된 이미지와 안개가 있는 이미지의 쌍이 필요하다. 하지만 실제로 이 데이터의 쌍을 구하기는 어렵다. 실제로는 각각의 이미지는 있지만 그 쌍이 맞지 않는다. 그리고 데이터를 만들어서 해도 문제가 있다. 안개가 없는 이미지에서 안개를 추가 한다고 해도 같은 장소의 이미지라도 안개의 정도에 따라서 다양한 안개 이미지가 나올 수 있다. 하지만 우리에게는 안개제거 알고리즘이 있다. 즉 안개 이미지에서 어떤 알고리즘을 사용해서 얻은 안개가 제거된 이미지를 가지고 알고리즘을 통해 안개가 제거된 이미지를 target값으로 실제 안개 이미지를 입력으로 해서 deep learning을 통해서 안개 제거 알고리즘을 구현한다.


요약하면 다음과 같다.

1. 안개가 없는 이미지와 안개가 있는 이미지의 쌍을 이룬 데이터를 구하기가 어렵다.
2. 우리에게는 안개 제거 알고리즘이 있다.
3. 따라서 안개가 없는 이미지에서 안개를 만드는 것 보다 안개가 있는 이미지에서 안개를 제거하는 것이 우리가 가진 도구이다.
4. 이 도구를 이용해서 안개 이미지(진짜 안개)와 안개가 제거된 이미지(알고리즘으로 안개가 제거된 이미지) 쌍을 만들 수 있다.
5. 이 데이터 쌍을 가지고 딥러닝 알고리즘으로 학습을 한다.

하지만 나는 이 아이디어 멍청한 방법이라고 생각한다. 그 이유는 아래와 같다.

1. 인터넷에서 안개 이미지를 검색해 본다면 같은 장소라도 안개의 정도(안개의 종류?)가 다양하다.
2. 하지만 우리가 가지고 있는 안개제거 알고리즘은 안개의 종류에 따라서 안개가 제거되는 기능이 없다.
3. 이것을 그림으로 나타낸다면 아래와 같다.

   ![](/assets/2018-03-01/4.png)

4. 즉, 같은 이미지라도 안개의 정도가 다르다면 안개가 제거된 후의 이미지가 다른 이미지로 간주해야 되지 않을까 생각한다. 그렇다면 내가 생각하는 이상적인 안개 제거 알고리즘은 아래와 같다.

   ![](/assets/2018-03-01/5.png)

5. 이미지가 같고 안개의 종류가 달라도 안개가 제거된 후의 이미지는 항상 같은 결과값이 나와야 하지 않을까 생각한다. 즉 초기 아이디어에서는 1대 1대응 함수를 만드는 거였다면 지금 제시한 아이디어는 n대 1대응 함수를 만드는 것이다.

6. 처음 아디이어가 멍청한 생각이라고 생각한 이유는 문제를 안개제거라는 주제를 가지고 단순히 데이터 쌍이 없으니 만들어서 안개제거 알고리즘을 만드는 것이라면 내가 제시한 아이디어는 안개 이미지가 가지는 특성**도** 고려해서 문제를 해결하려고 했다는 것이다.(조만간 [cycle GAN](https://arxiv.org/abs/1703.10593)을 변형시켜서 이 문제를 해결할 계획이다.) 즉, **안개 제거 문제가 가지고 있는 고유의 문제(나는 안개의 정도가 달라도 결과값은 일정해야 한다고 생각한다. 아마도 다른 사람마다 근본적으로 해결하고 싶은게 다를 수도 있을 것이다.)를 무시한 채 단순한 현상(안개 제거를 딥러닝으로 해결하려고 하니 데이터 쌍이 없네?)만을 가지고 문제를 해결하려고 하는데... (정리가 안된다... )**


자세히 생각해 보면 안개의 정도가 달라도 안개가 제거된 이미지가 항상 같게 나와야 한다는게 당연한 생각이 아닌가? 이래야지 여기서 생기는 문제(데이터 쌍이 없다는 문제)를 독창적인 방법으로 해결해야지 연구로서 가치가 있는게 아닐까?

#### 고찰

뭔가 하고 싶은 말이 있었는데 누구나 이해할 수 있게 설명이 안된다... 아니면 내가 틀린건가... 결국 내가 생각하는 연구는 어떤 현상을 보고 그 안에 있는 본질적인 원인을 찾아서 그것을 해결하는 **과정**이 연구라고 생각한다. 처음에는 나름 내 생각이 논리적으로 옳다고 생각했었는데 막상 글로 정리하다보니 아직 많이 부족하다고 느낀다...
