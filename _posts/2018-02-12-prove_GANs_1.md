---
title: GANs 논문 첫번째 증명 이해(4.1 Global Optimality 부분)
description:
categories:
 - proof
 - paper
tags:
 - GANs
---

요즘 딥러닝 분야를 공부하면서 느끼는 거지만 논문의 증명을 이해를 못해도 github 코드를 보면 알고리즘은 어떻게 해서든 구현을 할 수 있다. 그러면 나는 이 알고리즘을 안다고 생각한다. 하지만 진짜로 무언가를 할때는 부족함을 느낀다.

요즘 [Probability, Random Variables and Random Signal Principles](http://book.naver.com/bookdb/book_detail.nhn?bid=10199767) 책을 혼자서 공부하고 있다. 이 책을 공부하다가 [Generative Adversarial Networks(2014)](http://papers.nips.cc/paper/5423-generative-adversarial-nets.pdf) 논문을 다시 읽어 봤는데 전에는 이해 못했던 증명부분이 갑자기 이해가 되기 시작했다. 그래서 이해해본것을 한번 정리해보기로 했다.

## Global Optimality of p<sub>g</sub> = p<sub>data</sub>
### D*<sub>G</sub>(x) = p<sub>data</sub>(x)/(p<sub>data</sub>(x) + p<sub>g</sub>(x))
 ![](/assets/2018-02-12/1.png)

먼저 G를 고정하고 최적의 D를 구하면 위와 같다고 한다. 그 다음 저자는 왜 위과 같은 식이 되는지 증명을 해보았다.

 ![](/assets/2018-02-12/2.png)

위를 보면 p<sub>z</sub>(z) -> p<sub>g</sub>(x), g(z) -> x, dz -> dx와 같이 된다. 이게 성립하려면 이 알고리즘의 최종 목적인 G(z)값이 진짜 데이터와 같게 되는 경우이다. 이부분에 대한 설명은 없지만 수식을 보면 그렇게 이해할 수 밖에 없다. 여기서 p<sub>g</sub>(x)가 나오는데 이것은 g(z)=x가 된다고 해도 g(z)의해 나온 x의 확률분포가 p<sub>data</sub>(실제 데이터의 분포)와 같은지 다른지 모르기 때문에 새롭게 p<sub>g</sub>(g()함수를 통해 나온 x의 확률분포)를 정의한 것 같다. 그리고 dz가 dx로 바뀌어서 적분기호가 합쳐질 수 있는 이유는 모든 값의 합이기 때문인것 같다.

    처음에는 이부분이 이해할 수가 없었다. 입력데이터의 확률분포라는 것이 실제 알고리즘을 구현했을 때 전혀 안보이기?때문이다.  

y = a\*log(y) + b\*log(1-y) 미분해서 y에 대해서 정리하면 y = a/(a + b)가 된다. 여기서 a와 b, y는 확률값이기 때문에 값의 범위는 0~1까지 이고 이 범위에서 y값은 위로 볼록함수이다. 따라서 위로 볼록함수이기 때문에 미분했을때 0이 되는 지점이 최대값이 된다.

    여기서 V(G,D)는 범함수(함수들의 집합을 정의역으로 갖는 함수)로 이루어진 함수이다. G가 고정된 상태에서 V(G,D)를 최대화 하기 위해서는 범함수 D에 대해서 미분을 한다. 즉 V(G,D)를 D(x)에 대해 미분을 하지만 D(x)는 x와 상관 없다고 생각해야 한다. 따라서 적분기호는 x에 대한 연산이므로 무시해도 된다.(이 개념이 받아들이기 어려웠다.) 안에 있는 함수들의 미분함으로써 V(G,D)의 최대값이 될 때의 D(x)값을 구한것이다.

* 여기서 Supp(p<sub>data</sub>)와 Supp(p<sub>g</sub>)의 설명은 잘 모르겠다. 아마도 계속 공부하다보면 언젠간 알게될 것 같다.


### min V(G,D<sub>max</sub>) = -log4

위의 증명으로 V(G<sub>fixed</sub>,D)가 최대가 되는 D 값은 p<sub>data</sub>(x)/(p<sub>data</sub>(x) + p<sub>g</sub>(x))라는 것을 알았다. 다음 증명은 V(G,D<sub>max</sub>)의 최소값은 -log(4)이며, -log(4)가 되기 위해서는 p<sub>data</sub>(x) = p<sub>g</sub>(x)을 만족해야 된다는 내용이다.

 ![](/assets/2018-02-12/3.png)

이제 min V(G,D<sub>max</sub>) 값을 찾기 위해 위에서 구한 D*<sub>G</sub>(x) = p<sub>data</sub>(x)/(p<sub>data</sub>(x) + p<sub>g</sub>(x))를 대입한다. 1 - D*<sub>G</sub>(x)는 1 - p<sub>data</sub>(x)/(p<sub>data</sub>(x) + p<sub>g</sub>(x))가 되고 다시 정리하면 p<sub>g</sub>(x)/(p<sub>data</sub>(x) + p<sub>g</sub>(x))가 된다.

 ![](/assets/2018-02-12/4.png)

위의 식을 이해하기 위해서는 두가지 공식을 알아야 한다.

> KL(P\|\|Q) = E<sub>X~P</sub>[log(**P**(x)/**Q**(x))] = E<sub>X~P</sub>[log**P**(x) - log**Q**(x)]
JSD(P\|\|Q) = KL(**P**\|\|(**P** + **Q**)/2)/2 + KL(**Q**\|\|(**P** + **Q**)/2)/2

식(4)의 마지막 식을 정리해서 JSD(P\|\|Q)꼴을 하나 만들면 식(5)와 같은 결론이 나온다.

위의 식 KL(P\|\|Q)와 JSD(P\|\|Q)의 최소값이 0이고 그때 **P** = **Q** 가 성립한다. 따라서 min V(G,D<sub>max</sub>) = -log4 이다.
## 증명 요약
처음에는 max V(G<sub>fixed</sub>,D)를 만족하는 D(범함수)를 찾는다. 여기서 몇가지 가정이 있다고 추측했다.(이 추측이 가장 그럴싸한 추측인것 같다.)
1. G<sub>fixed</sub>(z)값은 x값 즉, 진짜 데이터의 값이 나온다.(아마도 G의 최종 목적이 진짜와 같게 만드는것 때문에 이런 가정을 한게 아닐까 생각한다.)
2. 위의 조건이 만족할때 x~p<sub>g</sub> 분포를 따른다. 하지만 **진짜 데이터의 분포 p<sub>data</sub>와 무슨 관계인지는 모른다.**

위의 가정을 가지고 max V(G<sub>fixed</sub>,D)를 만족하는 D<sub>max</sub>를 구한다.

그다음 위에서 구한 D<sub>max</sub>값을 가지고 min V(G,D<sub>max</sub>)을 만족하는 G(범함수)를 찾는다. 여기서 두 확률분포를 비교하는 수식인 **Kullback–Leibler divergence**와 **Jensen–Shannon divergence**을 가지고 V(G,D<sub>max</sub>)을 정리하면 p<sub>data</sub>와의 p<sub>g</sub> 관계가 나오게 됨과 동시에 Global Minimum이 존재한다는 것까지 보이게 된다.

    Global Minimum이 존재한다는 것은 머신러닝에서 새롭게 정의한 loss함수가 제대로 동작하는지 보여주기 위해 주로 쓰는 증명방법인 것 같다.


## 결론
* 위의 증명을 하나하나씩 무슨의미인지 알아가보면서 논리적으로 어떻게 변하는지 이해해보니 이론적인 설명이 EM 알고리즘과 매우 닮은것 같다.([이곳](http://sanghyukchun.github.io/70/)이 EM 알고리즘에 대한 설명이 좋은것 같다.) 공통점은 변수가 두개인데 동시에 업데이트를 할 수가 없어서 각각 업데이트해서 최적의 값을 찾는것이다. 차이점은 GANs은 두 변수가 범함수이고 반듯이 미분가능한 함수로 정의 되어야 한다는 것이다. 하지만 EM 알고리즘은 특별한 언급이 없다.(사실 EM 알고리즘은 대략적인 흐름만 알고 있다.)
* 위의 증명의 결론은 각각이(D와 G) 이상적인 값에 도달한다면 실제 데이터와 G(z)에 의에 생성된 데이터의 분포가 같다는 결론이다. 이러한 결론 때문에 GANs이 데이터의 확률분포를 학습한다고 말하는것 같다.
* 현재 딥러닝은 확률이라는 개념을 모르면 이해하기 어려운것같다. 다른 한편으로는 확률기반의 개념을 학습하기 때문에 그 자체가 한계인것 같다.
