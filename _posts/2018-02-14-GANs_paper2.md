---
title: 왜 GANs 논문에서 min[log(1-D(G(z)))]가 아니라 max[D(G(z))]를 했을까?
description:
categories:
 - paper
tags:
 - GANs
---




[Generative Adversarial Networks(2014)](http://papers.nips.cc/paper/5423-generative-adversarial-nets.pdf) 논문에서 나오는 수식과 실제 GANs을 구현한 코드를 보면 다르다. 얼마 전까지는 이 차이를 인식하지 못했지만 GANs 논문을 다시 읽어보다가 수식이 다르다는 것을 발견했다. 수식이 조금 다르지만 결국에는 같은 결론이 나온다는 것도 알았다.

왜 이런 실수를 했는지 생각해보면 그동안 인터넷에서 다운로드한 코드를 조금 수정해서 사용했기 때문인 것 같다. 다행인 것은 서로 다른 수식은 결국 같은 것을 의미하지만 나중을 위해서는 이런 식으로 연구하는 것을 멈춰야겠다.

### 문제의 부분

 ![](/assets/2018-02-14/1.png)

위의 식은 GANs의 가장 근본적인 식이다. D라는 범함수의 변화를 통해 V(D,G)를 최대화하고 동시에 G라는 범함수의 변화를 통해 V(D,G)를 최소화한다.

    Game Theory에서 Minimax 개념을 적용했다고 한다. 위의 수식은 직관적으로 이해하기 쉬우나 뭔가 숨겨진 개념이 더 존재하지 않을까 생각한다.

 ![](/assets/2018-02-14/2.png)

위의 부분을 읽어보면 log(1-D(G(z)))를 최소화하면 **saturated**?하기 때문에 D(G(z))를 최대화한다고 한다.

 ![](/assets/2018-02-14/3.png)

하지만 논문에서 소개한 알고리즘은 G를 업데이트할 때 D(G(z))를 사용하는 게 아니라 log(1-D(G(z)))를 사용했다.

그런데 GANs을 구현할 때 참고한 코드는 아래와 같다.(tensorflow for python)
```python
# loss for each network
eps = 1e-2
D_loss = tf.reduce_mean(-tf.log(D_real + eps) - tf.log(1 - D_fake + eps))
G_loss = tf.reduce_mean(-tf.log(D_fake + eps))

# trainable variables for each network
t_vars = tf.trainable_variables()
D_vars = [var for var in t_vars if 'D_' in var.name]
G_vars = [var for var in t_vars if 'G_' in var.name]

# optimizer for each network
D_optim = tf.train.AdamOptimizer(lr).minimize(D_loss, var_list=D_vars)
G_optim = tf.train.AdamOptimizer(lr).minimize(G_loss, var_list=G_vars)
```
출처 : [https://github.com/znxlwm/tensorflow-MNIST-GAN-DCGAN](https://github.com/znxlwm/tensorflow-MNIST-GAN-DCGAN)

위의 코드를 보면 D를 업데이트할 때는 minimum[-log(D(x) - log(1-D(G(z)))] 수식을 쓰고 G를 업데이트할 때는 minimum[-log(D(G(z)))] 수식을 썼다.

왜 이렇게 다를 수가 있는 걸까? 이제 이 수식이 의미하는 것을 분석해보자.

### 분석
#### gradient descent와 gradient ascent


[https://en.wikipedia.org/wiki/Gradient_descent](https://en.wikipedia.org/wiki/Gradient_descent) :

> To find a **local minimum** of a function using **gradient descent**, one takes steps proportional to the negative of the gradient (or of the approximate gradient) of the function at the current point.
If instead one takes steps proportional to the positive of the gradient, one approaches a **local maximum** of that function; the procedure is then known as **gradient ascent**.

위는 위키피디아의 설명이다. 즉, 둘의 근본척인 차이는 최댓값을 구하느냐 최솟값을 구하느냐이다. 따라서 두 수식은 다르다.

* gradient descent : $$\theta_{i+1} \leftarrow \theta_{i} - \triangle\theta_{i}$$

* gradient ascent : $$\theta_{i+1} \leftarrow \theta_{i} + \triangle\theta_{i}$$

수식은 위와 같다. 하지만 수식으로 보는 것보다 실제 그림으로 봐야 이해가 잘되는 것 같다.

 ![](/assets/2018-02-14/4.png)

총 4가지 경우로 나누었다. 아래로 볼록 함수(convex function)와 위로 볼록 함수(concave function)은 어떤 값이 수렴한다는 것을 설명할 때 사용되는 예시이다. gradient descent의 경우 convex function 함수의 경우 최솟값일 때의 $$\theta$$의 값으로 향한다. 이때 미분값은 0이 되고, 이 말은 더 이상의 변화가 없다는 말이다. 하지만 concave function인 경우 최소 값으로 향하는 지점의 미분값은 0이 아니다. 이 말은 어떤 특정값으로 수렴하지 않는다는 말이다. gradient ascent의 경우 반대로 생각하면 된다. 분석한 내용을 정리하면 다음과 같다.

    gradient descent/ascent는 최솟값/최댓값을 찾아주지만 그 값이 수렴한다는 것을 보장하지 않는다.

수렴을 보장하지 않는 다는 말이 실제 문제(적어도 컴퓨터를 쓰는 분야)에서 문제점이 무엇인지 생각해보자. 보통 수렴하지 않으면 발산한다고 한다. 수학에서 발산은 진동하는 경우와 무한대로 가는 경우가 있는데, 여기서는 무한대로 가는 경우 만 생각해 보겠다. 수학에서는 무한대라는 개념이 존재해서 수식으로 표현을 하지만, 실제 컴퓨터에서는 무한대가 나오는 경우 더이상 계산이 불가능하다. 무한대가 나오게 되면 컴퓨터에서는 주로 **inf**또는 **Nan**값이 출력되게 된다. 이러면 더이상 컴퓨터로는 계산이 불가능하다.

    수학 또는 이론으로 알고리즘을 공부할 때에는 개념 설명의 편의상 무한대라는 개념을 사용하지만 실제 알고리즘을 컴퓨터로 구현할 때는 무한대와 같은 상황을 피해야 한다.

위의 문장이 이 포스트의 질문 "왜 GANs 논문에서 min[log(1-D(G(z)))]가 아니라 max[D(G(z))]를 했을까?" 의 가장 그럴싸한 답이 아닐까 생각한다.


#### min[log(1-D(G(z)))]과 max[D(G(z))]의 차이  

먼저 위의 함수의 그래프를 눈으로 보자.

 ![](/assets/2018-02-14/5.png)

D(G(z))를 x로 치환하면 그 이유가 간단히 보인다. 위의 그림의 1번 경우인데 최솟값은 x가 1일 때 이다. 이떄 gradient descent 수식을 적용해보면

* gradient descent : $$\theta_{i+1} \leftarrow \theta_{i} - \triangle\theta_{i}$$
1. 우리가 원하는 최솟값은 x가 1일 때이다.
2. x가 1일 때, $$\triangle x = -\infty$$ 이므로  
3. $$x_{i+1} \leftarrow  x_{i} - \infty$$ 와 같이 바뀌며
4. 즉, $$x_{i+1} = \infty$$가 된다.

여기서 발견되는 문제는 두 가지이다.

1. x의 범위는 $${0\le x\le 1}$$ 인데(확률로 표현되기 때문에), gradient descent로 구한 값은 최종적으로 $$x = \infty$$ 가 나오게 된다.(gradient descent의 문제점)
2. 컴퓨터는 $$\triangle x = -\infty$$ 부분을 $$\triangle x$$ = **-inf**로 인식하기 때문에 계산이 불가능하다.(컴퓨터의 한계)  

여기서 저자가 아래와 같은 언급을 했는데,

> In this case, **log(1-D(G(z))) saturates**. Rather than training G to minimuze log(1-D(G(z))) we can train G to maximize log(D(G(z))).

* saturate의 영영 사전에서 동사의 의미 :

      to put a lot of something into a particular place, especially so that you could not add any more. -by longman dictionary

여기서 **log(1-D(G(z))) saturates** 의미는 위의 두 가지 상황을 한 단어로 표현한게 아닐까 생각한다.

따라서 $$min[ log(1-x)]$$에서 원하는 것은 $$x=1$$ 로 수렴하게 만드는 것이므로 $$max[ log(x)]$$로 바꿔도 $$x=1$$ 로 수렴하게 된다.(위의 그림 참조) 그래서 저자는 이러한 이유 때문에 식을 바꾼것이고, 위와 같은 이유로 실제 코드에서 GANs의 loss 함수를 **최대화 하고 최소화를 반복**하는게 아니라 **최소화와 최소화를 반복** 하는 방법으로 바꿔서 계산하는 것이다.


### 결론

나의 결론은 위의 앎의 과정에서 느낀것을 적은 것이다.

논문을 읽으면 항상 수학이라는 도구를 이용해서 알고리즘을 설명하지만 막상 실제로 컴퓨터로 알고리즘을 구현하려면 수식이 너무 많은 의미를 함축하기 때문에 앞이 깜깜했었다. 하지만 github에서 소스코드를 다운받아서 그 코드를 이해하면 되니깐 알고리즘을 구현하는 건 식은죽 먹기 였다. 그래서 어느순간부터 논문 읽기보다는 소스코드 이해위주로 공부했던 것 같다. 그러면 알고리즘은 많이 구현해서 실험도 해보고 결과도 얻었지만, 내 머리속에 남는게 하나도 없었던 것 같다. 과연 내가 그 알고리즘을 안다고 말할 수 있는 걸까?

공부한 것을 정리하고 글로 써서 블로그에 올리다 보니 위와 같은 생각이 머리속에서 계속 맴돌았다. 이 포스트를 다 쓰고 나서 생각해 본 것이지만, 연구란 것은 다른 사람의 연구를 보고 이해가 안되는 부분을 이해하려고 하다보면 문제점을 발견하게 되고 운이 좋으면 해결도 하고 더 운이 좋으면 그 해결책이 나만의 독창적인 생각이 되어서 논문을 써서 발표하는 이러한 과정의 반복이 아닐까 생각한다.
