---
title: 왜 GANs 논문에서 min[log(1-D(G(z)))]가 아니라 max[D(G(z))]를 했을까?
description:
categories:
 - paper
tags:
 - GANs
---




[Generative Adversarial Networks(2014)](http://papers.nips.cc/paper/5423-generative-adversarial-nets.pdf) 논문에서 나오는 수식과 실제  GANs을 구현한 코드를 보면 다르다. 얼마전까지는 이 차이를 인식하지 못했지만 GANs 논문을 다실 읽어보다가 수식이 다르다는 것을 발견했다. 수식이 조금 다르지만 결국에는 같은 결론이 나온다는 것도 알았다.

왜 이런 실수를 했는지 생각해보면 그동안 인터넷에서 다운받은 코드를 조금 수정해서 사용했기 때문인것 같다. 다행인것은 서로 다른 수식은 결국 같은것을 의미하지만 나중을 위해서는 이런 식으로 연구하는 것을 멈춰야 겠다.

### 문제의 부분

 ![](/assets/2018-02-14/1.png)

위의 식은 GANs의 가장 근본적인 식이다. D라는 범함수의 변화를 통해 V(D,G)를 최대화하고 동시에 G라는 범함수의 변화를 통해 V(D,G)를 최소화한다.

    Game Theory에서 Minimax 개념을 적용했다고 한다. 위의 수식은 직관적으로 이해하기 쉬우나 뭔가 숨겨진 개념이 더 존재하지 않을까 생각한다.

 ![](/assets/2018-02-14/2.png)

위의 부분을 읽어보면 log(1-D(G(z)))를 최소화하면 **saturated**?하기 때문에 D(G(z))를 최대화 한다고 한다.

 ![](/assets/2018-02-14/3.png)

하지만 논문에서 소개한 알고리즘은 G를 업데이트할 때 D(G(z))를 사용하는게 아니라 log(1-D(G(z)))를 사용했다.

그런데 GANs을 구현할 때 참고한 코드는 아래와 같다.(tensorflow for python)
```python
# loss for each network
eps = 1e-2
D_loss = tf.reduce_mean(-tf.log(D_real + eps) - tf.log(1 - D_fake + eps))
G_loss = tf.reduce_mean(-tf.log(D_fake + eps))

# trainable variables for each network
t_vars = tf.trainable_variables()
D_vars = [var for var in t_vars if 'D_' in var.name]
G_vars = [var for var in t_vars if 'G_' in var.name]

# optimizer for each network
D_optim = tf.train.AdamOptimizer(lr).minimize(D_loss, var_list=D_vars)
G_optim = tf.train.AdamOptimizer(lr).minimize(G_loss, var_list=G_vars)
```
출처 : [https://github.com/znxlwm/tensorflow-MNIST-GAN-DCGAN](https://github.com/znxlwm/tensorflow-MNIST-GAN-DCGAN)

위의 코드를 보면 D를 업데이트 할 때는 minimum[-log(D(x) - log(1-D(G(z)))] 수식을 쓰고 G를 업데이트 할 때는 maximum[-log(D(G(z)))] 수식을 썼다.

왜 이렇게 다를 수가 있는걸까? 이제 이 수식이 의미하는 것을 분석해보자.

### 분석




### 결론
